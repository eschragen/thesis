#cocacola
#shell
#extract unique tweets in English
vw_tweets = vw %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
vw_tweets_vector = VectorSource(vw_tweets)
vw_tweets_corpus = VCorpus(vw_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","vw","nestlé"))
return(corpus)
}
vw_clean_corpus = clean_corpus(vw_tweets_corpus)
#Word Frequency
vw_tdm = TermDocumentMatrix(vw_clean_corpus)
vw_m = as.matrix(vw_tdm)
vw_term_frequency = rowSums(vw_m)
vw_term_frequency = sort(vw_term_frequency, decreasing = TRUE)
top30_vw = data.frame(vw_term_frequency[1:30])
#barplot(vw_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
vw_hashtags = vw %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
vw_hashtags_clean = data.frame(strip(vw_hashtags))
vw_hashtags_vector = VectorSource(vw_hashtags_clean)
vw_hashtags_corpus = VCorpus(vw_hashtags_vector)
vw_clean_corpus_hashtags = clean_corpus(vw_hashtags_corpus)
vw_hashtags_tdm = TermDocumentMatrix(vw_clean_corpus_hashtags)
vw_m_hash = as.matrix(vw_hashtags_tdm)
vw_term_frequency_hash = rowSums(vw_m_hash)
vw_term_frequency_hash = sort(vw_term_frequency_hash, decreasing = TRUE)
top30_vw_hash = data.frame(vw_term_frequency_hash[1:30])
View(top30_vw)
View(top30_vw_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
unilever = read_csv("unilever.csv")
#unilever
#starbucks
#nestle
#mcdonalds
#ikea
#hm
#exxonmobil
#cocacola
#shell
#vw
#extract unique tweets in English
unilever_tweets = unilever %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
unilever_tweets_vector = VectorSource(unilever_tweets)
unilever_tweets_corpus = VCorpus(unilever_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","unilever","nestlé"))
return(corpus)
}
unilever_clean_corpus = clean_corpus(unilever_tweets_corpus)
#Word Frequency
unilever_tdm = TermDocumentMatrix(unilever_clean_corpus)
unilever_m = as.matrix(unilever_tdm)
unilever_term_frequency = rowSums(unilever_m)
unilever_term_frequency = sort(unilever_term_frequency, decreasing = TRUE)
top30_unilever = data.frame(unilever_term_frequency[1:30])
#barplot(unilever_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
unilever_hashtags = unilever %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
unilever_hashtags_clean = data.frame(strip(unilever_hashtags))
unilever_hashtags_vector = VectorSource(unilever_hashtags_clean)
unilever_hashtags_corpus = VCorpus(unilever_hashtags_vector)
unilever_clean_corpus_hashtags = clean_corpus(unilever_hashtags_corpus)
unilever_hashtags_tdm = TermDocumentMatrix(unilever_clean_corpus_hashtags)
unilever_m_hash = as.matrix(unilever_hashtags_tdm)
unilever_term_frequency_hash = rowSums(unilever_m_hash)
unilever_term_frequency_hash = sort(unilever_term_frequency_hash, decreasing = TRUE)
top30_unilever_hash = data.frame(unilever_term_frequency_hash[1:30])
View(top30_unilever)
View(top30_unilever_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
starbucks = read_csv("starbucks.csv")
#starbucks
#nestle
#mcdonalds
#ikea
#hm
#exxonmobil
#cocacola
#shell
#vw
#unilever
#extract unique tweets in English
starbucks_tweets = starbucks %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
starbucks_tweets_vector = VectorSource(starbucks_tweets)
starbucks_tweets_corpus = VCorpus(starbucks_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","starbucks","nestlé"))
return(corpus)
}
starbucks_clean_corpus = clean_corpus(starbucks_tweets_corpus)
#Word Frequency
starbucks_tdm = TermDocumentMatrix(starbucks_clean_corpus)
starbucks_m = as.matrix(starbucks_tdm)
starbucks_term_frequency = rowSums(starbucks_m)
starbucks_term_frequency = sort(starbucks_term_frequency, decreasing = TRUE)
top30_starbucks = data.frame(starbucks_term_frequency[1:30])
#barplot(starbucks_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
starbucks_hashtags = starbucks %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
starbucks_hashtags_clean = data.frame(strip(starbucks_hashtags))
starbucks_hashtags_vector = VectorSource(starbucks_hashtags_clean)
starbucks_hashtags_corpus = VCorpus(starbucks_hashtags_vector)
starbucks_clean_corpus_hashtags = clean_corpus(starbucks_hashtags_corpus)
starbucks_hashtags_tdm = TermDocumentMatrix(starbucks_clean_corpus_hashtags)
starbucks_m_hash = as.matrix(starbucks_hashtags_tdm)
starbucks_term_frequency_hash = rowSums(starbucks_m_hash)
starbucks_term_frequency_hash = sort(starbucks_term_frequency_hash, decreasing = TRUE)
top30_starbucks_hash = data.frame(starbucks_term_frequency_hash[1:30])
View(top30_starbucks)
View(top30_starbucks_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
nestle = read_csv("nestle.csv")
#nestle
#mcdonalds
#ikea
#hm
#exxonmobil
#cocacola
#shell
#vw
#unilever
#starbucks
#extract unique tweets in English
nestle_tweets = nestle %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
nestle_tweets_vector = VectorSource(nestle_tweets)
nestle_tweets_corpus = VCorpus(nestle_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","nestle","nestlé"))
return(corpus)
}
nestle_clean_corpus = clean_corpus(nestle_tweets_corpus)
#Word Frequency
nestle_tdm = TermDocumentMatrix(nestle_clean_corpus)
nestle_m = as.matrix(nestle_tdm)
nestle_term_frequency = rowSums(nestle_m)
nestle_term_frequency = sort(nestle_term_frequency, decreasing = TRUE)
top30_nestle = data.frame(nestle_term_frequency[1:30])
#barplot(nestle_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
nestle_hashtags = nestle %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
nestle_hashtags_clean = data.frame(strip(nestle_hashtags))
nestle_hashtags_vector = VectorSource(nestle_hashtags_clean)
nestle_hashtags_corpus = VCorpus(nestle_hashtags_vector)
nestle_clean_corpus_hashtags = clean_corpus(nestle_hashtags_corpus)
nestle_hashtags_tdm = TermDocumentMatrix(nestle_clean_corpus_hashtags)
nestle_m_hash = as.matrix(nestle_hashtags_tdm)
nestle_term_frequency_hash = rowSums(nestle_m_hash)
nestle_term_frequency_hash = sort(nestle_term_frequency_hash, decreasing = TRUE)
top30_nestle_hash = data.frame(nestle_term_frequency_hash[1:30])
View(top30_nestle)
View(top30_nestle_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
mcdonalds = read_csv("mcdonalds.csv")
#mcdonalds
#ikea
#hm
#exxonmobil
#cocacola
#shell
#vw
#unilever
#starbucks
#nestle
#extract unique tweets in English
mcdonalds_tweets = mcdonalds %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
mcdonalds_tweets_vector = VectorSource(mcdonalds_tweets)
mcdonalds_tweets_corpus = VCorpus(mcdonalds_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","mcdonalds","nestlé"))
return(corpus)
}
mcdonalds_clean_corpus = clean_corpus(mcdonalds_tweets_corpus)
#Word Frequency
mcdonalds_tdm = TermDocumentMatrix(mcdonalds_clean_corpus)
mcdonalds_m = as.matrix(mcdonalds_tdm)
mcdonalds_term_frequency = rowSums(mcdonalds_m)
mcdonalds_term_frequency = sort(mcdonalds_term_frequency, decreasing = TRUE)
top30_mcdonalds = data.frame(mcdonalds_term_frequency[1:30])
#barplot(mcdonalds_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
mcdonalds_hashtags = mcdonalds %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
mcdonalds_hashtags_clean = data.frame(strip(mcdonalds_hashtags))
mcdonalds_hashtags_vector = VectorSource(mcdonalds_hashtags_clean)
mcdonalds_hashtags_corpus = VCorpus(mcdonalds_hashtags_vector)
mcdonalds_clean_corpus_hashtags = clean_corpus(mcdonalds_hashtags_corpus)
mcdonalds_hashtags_tdm = TermDocumentMatrix(mcdonalds_clean_corpus_hashtags)
mcdonalds_m_hash = as.matrix(mcdonalds_hashtags_tdm)
mcdonalds_term_frequency_hash = rowSums(mcdonalds_m_hash)
mcdonalds_term_frequency_hash = sort(mcdonalds_term_frequency_hash, decreasing = TRUE)
top30_mcdonalds_hash = data.frame(mcdonalds_term_frequency_hash[1:30])
View(top30_mcdonalds)
View(top30_mcdonalds_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
ikea = read_csv("ikea.csv")
#ikea
#hm
#exxonmobil
#cocacola
#shell
#vw
#unilever
#starbucks
#nestle
#mcdonalds
#extract unique tweets in English
ikea_tweets = ikea %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
ikea_tweets_vector = VectorSource(ikea_tweets)
ikea_tweets_corpus = VCorpus(ikea_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","ikea","nestlé"))
return(corpus)
}
ikea_clean_corpus = clean_corpus(ikea_tweets_corpus)
#Word Frequency
ikea_tdm = TermDocumentMatrix(ikea_clean_corpus)
ikea_m = as.matrix(ikea_tdm)
ikea_term_frequency = rowSums(ikea_m)
ikea_term_frequency = sort(ikea_term_frequency, decreasing = TRUE)
top30_ikea = data.frame(ikea_term_frequency[1:30])
#barplot(ikea_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
ikea_hashtags = ikea %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
ikea_hashtags_clean = data.frame(strip(ikea_hashtags))
ikea_hashtags_vector = VectorSource(ikea_hashtags_clean)
ikea_hashtags_corpus = VCorpus(ikea_hashtags_vector)
ikea_clean_corpus_hashtags = clean_corpus(ikea_hashtags_corpus)
ikea_hashtags_tdm = TermDocumentMatrix(ikea_clean_corpus_hashtags)
ikea_m_hash = as.matrix(ikea_hashtags_tdm)
ikea_term_frequency_hash = rowSums(ikea_m_hash)
ikea_term_frequency_hash = sort(ikea_term_frequency_hash, decreasing = TRUE)
top30_ikea_hash = data.frame(ikea_term_frequency_hash[1:30])
View(top30_ikea)
View(top30_ikea_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
hm = read_csv("hm.csv")
#hm
#exxonmobil
#cocacola
#shell
#vw
#unilever
#starbucks
#nestle
#mcdonalds
#hm
#extract unique tweets in English
hm_tweets = hm %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
hm_tweets_vector = VectorSource(hm_tweets)
hm_tweets_corpus = VCorpus(hm_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","hm","nestlé"))
return(corpus)
}
hm_clean_corpus = clean_corpus(hm_tweets_corpus)
#Word Frequency
hm_tdm = TermDocumentMatrix(hm_clean_corpus)
hm_m = as.matrix(hm_tdm)
hm_term_frequency = rowSums(hm_m)
hm_term_frequency = sort(hm_term_frequency, decreasing = TRUE)
top30_hm = data.frame(hm_term_frequency[1:30])
#barplot(hm_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
hm_hashtags = hm %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
hm_hashtags_clean = data.frame(strip(hm_hashtags))
hm_hashtags_vector = VectorSource(hm_hashtags_clean)
hm_hashtags_corpus = VCorpus(hm_hashtags_vector)
hm_clean_corpus_hashtags = clean_corpus(hm_hashtags_corpus)
hm_hashtags_tdm = TermDocumentMatrix(hm_clean_corpus_hashtags)
hm_m_hash = as.matrix(hm_hashtags_tdm)
hm_term_frequency_hash = rowSums(hm_m_hash)
hm_term_frequency_hash = sort(hm_term_frequency_hash, decreasing = TRUE)
top30_hm_hash = data.frame(hm_term_frequency_hash[1:30])
View(top30_hm)
View(top30_hm_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
exxonmobil = read_csv("exxonmobil.csv")
#cocacola
#shell
#vw
#unilever
#starbucks
#nestle
#mcdonalds
#hm
#extract unique tweets in English
exxonmobil_tweets = exxonmobil %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
exxonmobil_tweets_vector = VectorSource(exxonmobil_tweets)
exxonmobil_tweets_corpus = VCorpus(exxonmobil_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","exxonmobil","nestlé"))
return(corpus)
}
exxonmobil_clean_corpus = clean_corpus(exxonmobil_tweets_corpus)
#Word Frequency
exxonmobil_tdm = TermDocumentMatrix(exxonmobil_clean_corpus)
exxonmobil_m = as.matrix(exxonmobil_tdm)
exxonmobil_term_frequency = rowSums(exxonmobil_m)
exxonmobil_term_frequency = sort(exxonmobil_term_frequency, decreasing = TRUE)
top30_exxonmobil = data.frame(exxonmobil_term_frequency[1:30])
#barplot(exxonmobil_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
exxonmobil_hashtags = exxonmobil %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
exxonmobil_hashtags_clean = data.frame(strip(exxonmobil_hashtags))
exxonmobil_hashtags_vector = VectorSource(exxonmobil_hashtags_clean)
exxonmobil_hashtags_corpus = VCorpus(exxonmobil_hashtags_vector)
exxonmobil_clean_corpus_hashtags = clean_corpus(exxonmobil_hashtags_corpus)
exxonmobil_hashtags_tdm = TermDocumentMatrix(exxonmobil_clean_corpus_hashtags)
exxonmobil_m_hash = as.matrix(exxonmobil_hashtags_tdm)
exxonmobil_term_frequency_hash = rowSums(exxonmobil_m_hash)
exxonmobil_term_frequency_hash = sort(exxonmobil_term_frequency_hash, decreasing = TRUE)
top30_exxonmobil_hash = data.frame(exxonmobil_term_frequency_hash[1:30])
View(top30_exxonmobil)
View(top30_exxonmobil_hash)
library(tidyverse)
library(readxl)
library(dplyr)
library(tm)
library(qdap)
#upload & combine csv files
setwd("~/GitHub/twint/outputs")
cocacola = read_csv("cocacola.csv")
#cocacola
#shell
#vw
#unilever
#starbucks
#nestle
#mcdonalds
#hm
#exxonmobil
#extract unique tweets in English
cocacola_tweets = cocacola %>%
filter(language == "en") %>%
select(tweet) %>%
distinct()
#Preprocessing
cocacola_tweets_vector = VectorSource(cocacola_tweets)
cocacola_tweets_corpus = VCorpus(cocacola_tweets_vector)
clean_corpus = function(corpus){
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
#corpus = tm_map(corpus, stemDocument, language = "english")   ##Not relevant for Scraping
corpus = tm_map(corpus, removeWords,c(stopwords("en"),"greenwashing","cocacola","nestlé"))
return(corpus)
}
cocacola_clean_corpus = clean_corpus(cocacola_tweets_corpus)
#Word Frequency
cocacola_tdm = TermDocumentMatrix(cocacola_clean_corpus)
cocacola_m = as.matrix(cocacola_tdm)
cocacola_term_frequency = rowSums(cocacola_m)
cocacola_term_frequency = sort(cocacola_term_frequency, decreasing = TRUE)
top30_cocacola = data.frame(cocacola_term_frequency[1:30])
#barplot(cocacola_term_frequency[1:30], col = "tan", las = 2)
#Hashtags
cocacola_hashtags = cocacola %>%
filter(language == "en") %>%
select(hashtags) %>%
distinct()
cocacola_hashtags_clean = data.frame(strip(cocacola_hashtags))
cocacola_hashtags_vector = VectorSource(cocacola_hashtags_clean)
cocacola_hashtags_corpus = VCorpus(cocacola_hashtags_vector)
cocacola_clean_corpus_hashtags = clean_corpus(cocacola_hashtags_corpus)
cocacola_hashtags_tdm = TermDocumentMatrix(cocacola_clean_corpus_hashtags)
cocacola_m_hash = as.matrix(cocacola_hashtags_tdm)
cocacola_term_frequency_hash = rowSums(cocacola_m_hash)
cocacola_term_frequency_hash = sort(cocacola_term_frequency_hash, decreasing = TRUE)
top30_cocacola_hash = data.frame(cocacola_term_frequency_hash[1:30])
View(top30_cocacola)
View(top30_cocacola_hash)
